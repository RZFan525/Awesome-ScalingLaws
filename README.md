# Awesome-ScalingLaws [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![](https://img.shields.io/badge/build-welcome%20to%20contribute!-brightgreen) [![GitHub Stars](https://img.shields.io/github/stars/RZFan525/Awesome-ScalingLaws?style=social)](https://github.com/RZFan525/Awesome-ScalingLaws/stargazers)

A curated list of awesome resources dedicated to Scaling Laws for LLMs (Large Language Models).

**Contributing**: Please feel free to make *[pull requests](https://github.com/RZFan525/Awesome-ScalingLaws/pulls)*.

What's the Scaling Laws of LLMs? It means that the test loss of a model can be predicted using a power-law when performance is limited by only either the number of model's parameters, the dataset size, or the optimally allocated compute budget.


## Papers
1. **Deep Learning Scaling is Predictable, Empirically**

   *Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou* [[paper]](https://arxiv.org/abs/1712.00409) Arxiv 2017.12

2. **Scaling Laws for Neural Language Models**

   *Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei* [[paper]](https://arxiv.org/abs/2001.08361) Arxiv 2020.01
   
3. **The Computational Limits of Deep Learning**

   *Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, Gabriel F. Manso* [[paper]](https://arxiv.org/abs/2007.05558) Arxiv 2020.07
   
4. **Scaling Laws for Autoregressive Generative Modeling**

   *Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish* [[paper]](https://arxiv.org/abs/2010.14701) Arxiv 2020.10
   
5. **Scaling Laws for Transfer**

   *Danny Hernandez, Jared Kaplan, Tom Henighan, Sam McCandlish* [[paper]](https://arxiv.org/abs/2102.01293) Arxiv 2021.02

6. **Explaining Neural Scaling Laws**
   
   *Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, Utkarsh Sharma* [[paper]](https://arxiv.org/abs/2102.06701) Arxiv 2021.02

7. **Universal scaling laws in the gradient descent training of neural networks**

   *Maksim Velikanov, Dmitry Yarotsky* [[paper]](https://arxiv.org/abs/2105.00507) Arxiv 2021.05

8. **Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization**

   *Divya Shanmugam, Samira Shabanian, Fernando Diaz, Mich√®le Finck, Asia Biega* [[paper]](https://arxiv.org/abs/2107.08096) ACM Conference on Fairness 2021.07
   
9. **Scaling Laws for Deep Learning**
   
   *Jonathan S. Rosenfeld* [[paper]](https://arxiv.org/abs/2108.07686) MIT PhD thesis 2021.08

10. **A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?**

    *Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji Suzuki, Shin-ichi Maeda, Kohei Hayashi* [[paper]](https://arxiv.org/abs/2108.11018) Arxiv 2021.08
 
11. **Scaling Laws for Neural Machine Translation**

    *Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, Colin Cherry* [[paper]](https://arxiv.org/abs/2109.07740) Arxiv 2021.09

12. **Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers**  
    
    *Gabriele Prato, Simon Guiroy, Ethan Caballero, Irina Rish, Sarath Chandar* [[paper]](https://arxiv.org/abs/2110.06990) Arxiv 2021.10

13. **Scaling Law for Recommendation Models: Towards General-purpose User Representations**

    *Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihlen Ramstrom, Jisu Jeong, Jung-Woo Ha, Kyung-Min Kim* [[paper]](https://arxiv.org/abs/2111.11294) AAAI2023 2021.11
   
14. **Unified Scaling Laws for Routed Language Models**

    *Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, Karen Simonyan* [[paper]](https://arxiv.org/abs/2202.01169) ICML2022 2022.02
    
15. **Data Scaling Laws in NMT: The Effect of Noise and Architecture**

    *Yamini Bansal, Behrooz Ghorbani, Ankush Garg, Biao Zhang, Maxim Krikun, Colin Cherry, Behnam Neyshabur, Orhan Firat* [[paper]](https://arxiv.org/abs/2202.01994) Arxiv 2022.02

16. **Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments**

    *Maor Ivgi, Yair Carmon, Jonathan Berant* [[paper]](https://arxiv.org/abs/2202.06387) EMNLP2022 2022.02


17. **Training Compute-Optimal Large Language Models**

    *Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre* [[paper]](https://arxiv.org/abs/2203.15556) Arxiv 2022.03
 
18. **Scaling Laws and Interpretability of Learning from Repeated Data**

    *Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, Sam McCandlish* [[paper]](https://arxiv.org/abs/2205.10487) Arxiv 2022.05

19. **Beyond neural scaling laws: beating power law scaling via data pruning**

    *Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, Ari S. Morcos* [[paper]](https://arxiv.org/abs/2206.14486) NeurIPS2022 2022.06
 
20. **Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?**

    *Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler* [[paper]](https://arxiv.org/abs/2207.10551) Arxiv 2022.07
    
21. **Understanding Scaling Laws for Recommendation Models**

    *Newsha Ardalani, Carole-Jean Wu, Zeliang Chen, Bhargav Bhushanam, Adnan Aziz* [[paper]](https://arxiv.org/abs/2208.08489) Arxiv 2022.08

22. **Revisiting Neural Scaling Laws in Language and Vision**

    *Ibrahim Alabdulmohsin, Behnam Neyshabur, Xiaohua Zhai* [[paper]](https://arxiv.org/abs/2209.06640) NeurIPS2022 2022.09
    
23. **Scaling Laws For Deep Learning Based Image Reconstruction**

    *Tobit Klug, Reinhard Heckel* [[paper]](https://arxiv.org/abs/2209.13435) ICLR2023 2022.09
    
24. **Scaling Laws for a Multi-Agent Reinforcement Learning Model**

    *Oren Neumann, Claudius Gros* [[paper]](https://arxiv.org/abs/2210.00849) Arxiv 2022.10
    
25. **How Much Data Are Augmentations Worth? An Investigation into Scaling Laws, Invariance, and Implicit Regularization**

    *Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Goldstein, Andrew Gordon Wilson* [[paper]](https://arxiv.org/abs/2210.06441) ICLR2023 2022.10
    
26. **Scaling Laws for Reward Model Overoptimization**

    *Leo Gao, John Schulman, Jacob Hilton* [[paper]](https://arxiv.org/abs/2210.10760) Arxiv 2022.10
 
27. **Transcending Scaling Laws with 0.1% Extra Compute**

    *Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani* [[paper]](https://arxiv.org/abs/2210.11399) Arxiv 2022.10

28. **Scaling Laws Beyond Backpropagation**

    *Matthew J. Filipovich, Alessandro Cappelli, Daniel Hesslow, Julien Launay* [[paper]](https://arxiv.org/abs/2210.14593) NeurIPS2022 2022.10
    
29. **Broken Neural Scaling Laws**

    *Ethan Caballero, Kshitij Gupta, Irina Rish, David Krueger* [[paper]](https://arxiv.org/abs/2210.14891) ICLR2023 2022.10
    
30. **A Solvable Model of Neural Scaling Laws**
    
    *Alexander Maloney, Daniel A. Roberts, James Sully* [[paper]](https://arxiv.org/abs/2210.16859) Arxiv 2022.10
    
31. **An Information-Theoretic Analysis of Compute-Optimal Neural Scaling Laws**

    *Hong Jun Jeon, Benjamin Van Roy* [[paper]](https://arxiv.org/abs/2212.01365) Arxiv 2022.12

32. **Reproducible scaling laws for contrastive language-image learning**

    *Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, Jenia Jitsev* [[paper]](https://arxiv.org/abs/2212.07143) Arxiv 2022.12
    
33. **The case for 4-bit precision: k-bit Inference Scaling Laws**

    *Tim Dettmers, Luke Zettlemoyer* [[paper]](https://arxiv.org/abs/2212.09720) Arxiv 2022.12
    
34. **Scaling Laws for Generative Mixed-Modal Language Models**

    *Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, Luke Zettlemoyer* [[paper]](https://arxiv.org/abs/2301.03728) Arxiv 2023.01
    
35. **Scaling laws for single-agent reinforcement learning**

    *Jacob Hilton, Jie Tang, John Schulman* [[paper]](https://arxiv.org/abs/2301.13442) Arxiv 2023.01
    
36. **Data pruning and neural scaling laws: fundamental limitations of score-based algorithms**
  
    *Fadhel Ayed, Soufiane Hayou* [[paper]](https://arxiv.org/abs/2302.06960) Arxiv 2023.02
 
37. **Scaling Laws for Multilingual Neural Machine Translation**

    *Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, Orhan Firat* [[paper]](https://arxiv.org/abs/2302.09650) Arxiv 2023.02

38. **GPT-4 Technical Report**

    *OpenAI* [[paper]](https://arxiv.org/abs/2303.08774) Arxiv 2023.03

