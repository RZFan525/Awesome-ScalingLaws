# Awesome-ScalingLaws [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
![](https://img.shields.io/badge/build-welcome%20to%20contribute!-brightgreen) [![GitHub Stars](https://img.shields.io/github/stars/RZFan525/Awesome-ScalingLaws?style=social)](https://github.com/RZFan525/Awesome-ScalingLaws/stargazers)

A curated list of awesome resources dedicated to Scaling Laws for LLMs.

**Contributing**: Please feel free to make *[pull requests](https://github.com/RZFan525/Awesome-ScalingLaws/pulls)*.

## Papers
1. **Deep Learning Scaling is Predictable, Empirically**

   *Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, Yanqi Zhou* [[paper]](https://arxiv.org/abs/1712.00409) Arxiv 2017.12

2. **Scaling Laws for Neural Language Models**

   *Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei* [[paper]](https://arxiv.org/abs/2001.08361) Arxiv 2020.01
   
3. **The Computational Limits of Deep Learning**

   *Neil C. Thompson, Kristjan Greenewald, Keeheon Lee, Gabriel F. Manso* [[paper]](https://arxiv.org/abs/2007.05558) Arxiv 2020.07
   
4. **Scaling Laws for Autoregressive Generative Modeling**

   *Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, Sam McCandlish* [[paper]](https://arxiv.org/abs/2010.14701) Arxiv 2020.10
   
5. **Scaling Laws for Transfer**

   *Danny Hernandez, Jared Kaplan, Tom Henighan, Sam McCandlish* [[paper]](https://arxiv.org/abs/2102.01293) Arxiv 2021.02

6. **Explaining Neural Scaling Laws**
   
   *Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, Utkarsh Sharma* [[paper]](https://arxiv.org/abs/2102.06701) Arxiv 2021.02

7. **Universal scaling laws in the gradient descent training of neural networks**

   *Maksim Velikanov, Dmitry Yarotsky* [[paper]](https://arxiv.org/abs/2105.00507) Arxiv 2021.05

8. **Learning to Limit Data Collection via Scaling Laws: A Computational Interpretation for the Legal Principle of Data Minimization**

   *Divya Shanmugam, Samira Shabanian, Fernando Diaz, Mich√®le Finck, Asia Biega* [[paper]](https://arxiv.org/abs/2107.08096) ACM Conference on Fairness 2021.07
   
9. **Scaling Laws for Deep Learning**
   
   *Jonathan S. Rosenfeld* [[paper]](https://arxiv.org/abs/2108.07686) MIT PhD thesis 2021.08

10. **A Scaling Law for Synthetic-to-Real Transfer: How Much Is Your Pre-training Effective?**

    *Hiroaki Mikami, Kenji Fukumizu, Shogo Murai, Shuji Suzuki, Yuta Kikuchi, Taiji Suzuki, Shin-ichi Maeda, Kohei Hayashi* [[paper]](https://arxiv.org/abs/2108.11018) Arxiv 2021.08
 
11. **Scaling Laws for Neural Machine Translation**

    *Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, Colin Cherry* [[paper]](https://arxiv.org/abs/2109.07740) Arxiv 2021.09

12. **Scaling Laws for the Few-Shot Adaptation of Pre-trained Image Classifiers**  
    
    *Gabriele Prato, Simon Guiroy, Ethan Caballero, Irina Rish, Sarath Chandar* [[paper]](https://arxiv.org/abs/2110.06990) Arxiv 2021.10

13. **Scaling Law for Recommendation Models: Towards General-purpose User Representations**

    *Kyuyong Shin, Hanock Kwak, Su Young Kim, Max Nihlen Ramstrom, Jisu Jeong, Jung-Woo Ha, Kyung-Min Kim* [[paper]](https://arxiv.org/abs/2111.11294) AAAI2023 2021.11
   
14. **Unified Scaling Laws for Routed Language Models**

    *Aidan Clark, Diego de las Casas, Aurelia Guy, Arthur Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, Tom Hennigan, Matthew Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, David Budden, Laurent Sifre, Simon Osindero, Oriol Vinyals, Jack Rae, Erich Elsen, Koray Kavukcuoglu, Karen Simonyan* [[paper]](https://arxiv.org/abs/2202.01169) ICML2022 2022.02

15. **Training Compute-Optimal Large Language Models**

   *Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre* [[paper]](https://arxiv.org/abs/2203.15556) Arxiv 2022.03
 
16. **Scaling Laws and Interpretability of Learning from Repeated Data**

   *Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris Olah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, Sam McCandlish* [[paper]](https://arxiv.org/abs/2205.10487) Arxiv 2022.05
 
17. **Transcending Scaling Laws with 0.1% Extra Compute**

   *Yi Tay, Jason Wei, Hyung Won Chung, Vinh Q. Tran, David R. So, Siamak Shakeri, Xavier Garcia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha Chowdhery, Denny Zhou, Donald Metzler, Slav Petrov, Neil Houlsby, Quoc V. Le, Mostafa Dehghani* [[paper]](https://arxiv.org/abs/2210.11399) Arxiv 2022.10
 
18. **Scaling Laws for Multilingual Neural Machine Translation**

   *Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, Orhan Firat* [[paper]](https://arxiv.org/abs/2302.09650) Arxiv 2023.02

19. **GPT-4 Technical Report**

    *OpenAI* [[paper]](https://arxiv.org/abs/2303.08774) Arxiv 2023.03

